{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "4XgQ0dd1ydf_"
      },
      "outputs": [],
      "source": [
        "# @title Install Dependencies\n",
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "!python -m pip install -e detectron2\n",
        "!pip install av\n",
        "\n",
        "!pip install ninja\n",
        "!git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing\n",
        "\n",
        "!pip install -q mediapipe==0.10.0\n",
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
        "\n",
        "!pip install opencv-python torchgeometry Pillow tqdm tensorboardX scikit-image scipy\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import shutil\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from PIL import Image, ImageOps\n",
        "import glob\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "shutil.copy('/content/drive/MyDrive/viton/densepose/mod_apply_net.py', '/content/detectron2/projects/DensePose/mod_apply_net.py')\n",
        "\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
        "\n",
        "\n",
        "def create_keypoints_json(rgb_image, body_detection_result, hands_detection_result, image_name):\n",
        "  pose_landmarks_list = body_detection_result.pose_landmarks\n",
        "  hand_landmarks_list = hands_detection_result.hand_landmarks\n",
        "  handedness_list = hands_detection_result.handedness\n",
        "\n",
        "  # Loop through the detected poses to visualize.\n",
        "  for idx in range(len(pose_landmarks_list)):\n",
        "    pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "    kp_1_x = ((pose_landmarks[11].x * rgb_image.shape[1]) + (pose_landmarks[12].x * rgb_image.shape[1])) / 2\n",
        "    kp_1_y = ((pose_landmarks[11].y * rgb_image.shape[0]) + (pose_landmarks[12].y * rgb_image.shape[0])) / 2\n",
        "    kp_1_v = (pose_landmarks[11].visibility + pose_landmarks[12].visibility) / 2\n",
        "\n",
        "    kp_8_x = ((pose_landmarks[23].x * rgb_image.shape[1]) + (pose_landmarks[24].x * rgb_image.shape[1])) / 2\n",
        "    kp_8_y = ((pose_landmarks[23].y * rgb_image.shape[0]) + (pose_landmarks[24].y * rgb_image.shape[0])) / 2\n",
        "    kp_8_v = (pose_landmarks[23].visibility + pose_landmarks[24].visibility) / 2\n",
        "\n",
        "    pose_keypoints_2d = [\n",
        "    pose_landmarks[0].x * rgb_image.shape[1], pose_landmarks[0].y * rgb_image.shape[0], pose_landmarks[0].visibility,\n",
        "    kp_1_x, kp_1_y, kp_1_v,\n",
        "    pose_landmarks[12].x * rgb_image.shape[1], pose_landmarks[12].y * rgb_image.shape[0], pose_landmarks[12].visibility,\n",
        "    pose_landmarks[14].x * rgb_image.shape[1], pose_landmarks[14].y * rgb_image.shape[0], pose_landmarks[14].visibility,\n",
        "    pose_landmarks[16].x * rgb_image.shape[1], pose_landmarks[16].y * rgb_image.shape[0], pose_landmarks[16].visibility,\n",
        "    pose_landmarks[11].x * rgb_image.shape[1], pose_landmarks[11].y * rgb_image.shape[0], pose_landmarks[11].visibility,\n",
        "    pose_landmarks[13].x * rgb_image.shape[1], pose_landmarks[13].y * rgb_image.shape[0], pose_landmarks[13].visibility,\n",
        "    pose_landmarks[15].x * rgb_image.shape[1], pose_landmarks[15].y * rgb_image.shape[0], pose_landmarks[15].visibility,\n",
        "    kp_8_x, kp_8_y, kp_8_v,\n",
        "    pose_landmarks[24].x * rgb_image.shape[1], pose_landmarks[24].y * rgb_image.shape[0], pose_landmarks[24].visibility,\n",
        "    pose_landmarks[26].x * rgb_image.shape[1], pose_landmarks[26].y * rgb_image.shape[0], pose_landmarks[26].visibility,\n",
        "    pose_landmarks[28].x * rgb_image.shape[1], pose_landmarks[28].y * rgb_image.shape[0], pose_landmarks[28].visibility,\n",
        "    pose_landmarks[23].x * rgb_image.shape[1], pose_landmarks[23].y * rgb_image.shape[0], pose_landmarks[23].visibility,\n",
        "    pose_landmarks[25].x * rgb_image.shape[1], pose_landmarks[25].y * rgb_image.shape[0], pose_landmarks[25].visibility,\n",
        "    pose_landmarks[27].x * rgb_image.shape[1], pose_landmarks[27].y * rgb_image.shape[0], pose_landmarks[27].visibility,\n",
        "    pose_landmarks[5].x * rgb_image.shape[1], pose_landmarks[5].y * rgb_image.shape[0], pose_landmarks[5].visibility,\n",
        "    pose_landmarks[2].x * rgb_image.shape[1], pose_landmarks[2].y * rgb_image.shape[0], pose_landmarks[2].visibility,\n",
        "    pose_landmarks[8].x * rgb_image.shape[1], pose_landmarks[8].y * rgb_image.shape[0], pose_landmarks[8].visibility,\n",
        "    pose_landmarks[7].x * rgb_image.shape[1], pose_landmarks[7].y * rgb_image.shape[0], pose_landmarks[7].visibility,\n",
        "    pose_landmarks[29].x * rgb_image.shape[1], pose_landmarks[29].y * rgb_image.shape[0], pose_landmarks[29].visibility,\n",
        "    pose_landmarks[31].x * rgb_image.shape[1], pose_landmarks[31].y * rgb_image.shape[0], pose_landmarks[31].visibility,\n",
        "    pose_landmarks[27].x * rgb_image.shape[1], pose_landmarks[27].y * rgb_image.shape[0], pose_landmarks[27].visibility,\n",
        "    pose_landmarks[30].x * rgb_image.shape[1], pose_landmarks[30].y * rgb_image.shape[0], pose_landmarks[30].visibility,\n",
        "    pose_landmarks[32].x * rgb_image.shape[1], pose_landmarks[32].y * rgb_image.shape[0], pose_landmarks[32].visibility,\n",
        "    pose_landmarks[28].x * rgb_image.shape[1], pose_landmarks[28].y * rgb_image.shape[0], pose_landmarks[28].visibility\n",
        "    ]\n",
        "\n",
        "\n",
        "  # Loop through the detected hands to visualize.\n",
        "\n",
        "  if len(hand_landmarks_list) != 0:\n",
        "\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "      hand_landmarks = hand_landmarks_list[idx]\n",
        "      handedness = handedness_list[idx]\n",
        "\n",
        "      hand_right_keypoints_2d = []\n",
        "      hand_left_keypoints_2d = []\n",
        "      if handedness[0].category_name == 'Right':\n",
        "        for lmk in hand_landmarks:\n",
        "          hand_right_keypoints_2d.append(lmk.x * rgb_image.shape[1])\n",
        "          hand_right_keypoints_2d.append(lmk.y * rgb_image.shape[0])\n",
        "          hand_right_keypoints_2d.append(lmk.visibility)\n",
        "      if handedness[0].category_name == 'Left':\n",
        "        for lmk in hand_landmarks:\n",
        "          hand_left_keypoints_2d.append(lmk.x * rgb_image.shape[1])\n",
        "          hand_left_keypoints_2d.append(lmk.y * rgb_image.shape[0])\n",
        "          hand_left_keypoints_2d.append(lmk.visibility)\n",
        "\n",
        "  else:\n",
        "    hand_right_keypoints_2d = []\n",
        "    hand_left_keypoints_2d = []\n",
        "\n",
        "\n",
        "\n",
        "  json_dict = {\n",
        "        \"version\":1.3,\n",
        "        \"people\":[\n",
        "            {\n",
        "                \"person_id\":[-1],\n",
        "                \"pose_keypoints_2d\": pose_keypoints_2d,\n",
        "                \"face_keypoints_2d\":[],\n",
        "                \"hand_left_keypoints_2d\": hand_left_keypoints_2d,\n",
        "                \"hand_right_keypoints_2d\": hand_right_keypoints_2d,\n",
        "                \"pose_keypoints_3d\":[],\n",
        "                \"face_keypoints_3d\":[],\n",
        "                \"hand_left_keypoints_3d\":[],\n",
        "                \"hand_right_keypoints_3d\":[]\n",
        "            }\n",
        "        ]\n",
        "  }\n",
        "\n",
        "  json_obj = json.dumps(json_dict)\n",
        "\n",
        "  with open(f'/content/drive/MyDrive/viton/HR/data/test/openpose_json/{image_name}_keypoints.json', 'w') as f:\n",
        "    f.write(json_obj)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Person Image\n",
        "#Please select images which you want to upload\n",
        "shutil.rmtree('/content/drive/MyDrive/viton/HR/data/test/image')\n",
        "\n",
        "!mkdir /content/drive/MyDrive/viton/HR/data/test/image\n",
        "%cd /content/drive/MyDrive/viton/HR/data/test/image\n",
        "\n",
        "print(\"Upload Person Image: \")\n",
        "uploaded = files.upload()\n",
        "image_name = list(uploaded.keys())[0][:-4]\n",
        "\n",
        "\n",
        "image = Image.open(f'/content/drive/MyDrive/viton/HR/data/test/image/{image_name}.jpg')\n",
        "image = ImageOps.fit(image, (768, 1024))\n",
        "image.save(f'/content/drive/MyDrive/viton/HR/data/test/image/{image_name}.jpg')\n",
        "\n",
        "%cd /content\n",
        "\n",
        "cloth_dir = '/content/drive/MyDrive/viton/HR/data/test/cloth'\n",
        "cloth_ids = os.listdir(cloth_dir)\n",
        "print(f\"Choose Cloth id from: {cloth_ids}\")\n",
        "cloth_id = input()\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/viton/HR/data/test_pairs.txt', 'w')  as f:\n",
        "  f.write(f'{image_name}.jpg {cloth_id}.jpg')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dDyGf3n3kr2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply Virtual-Try-On\n",
        "%cd /content/detectron2/projects/DensePose\n",
        "!python mod_apply_net.py show /content/drive/MyDrive/viton/densepose/configs/densepose_rcnn_R_50_FPN_s1x.yaml /content/drive/MyDrive/viton/densepose/models/model_final_162be9.pkl /content/drive/MyDrive/viton/HR/data/test/image dp_segm --output output.jpg\n",
        "\n",
        "\n",
        "dp_img = Image.open(glob.glob('/content/detectron2/projects/DensePose/*.jpg')[0])\n",
        "dp_img.save(f'/content/drive/MyDrive/viton/HR/data/test/image-densepose/{image_name}.jpg')\n",
        "\n",
        "op_img = Image.open('/content/drive/MyDrive/viton/HR/data/test/openpose_img/sample_rendered.png')\n",
        "op_img.save(f'/content/drive/MyDrive/viton/HR/data/test/openpose_img/{image_name}_rendered.png')\n",
        "\n",
        "%cd /content/Self-Correction-Human-Parsing\n",
        "!python3 simple_extractor.py --dataset 'lip' --model-restore /content/drive/MyDrive/viton/schp/model/exp-schp-201908261155-lip.pth --input-dir /content/drive/MyDrive/viton/HR/data/test/image --output-dir /content/drive/MyDrive/viton/HR/data/test/image-parse-v3\n",
        "%cd /content\n",
        "\n",
        "shutil.copy(f'/content/drive/MyDrive/viton/HR/data/test/cloth/{cloth_id}.jpg', f'/content/drive/MyDrive/viton/HR/data/test/cloth/{image_name}.jpg')\n",
        "shutil.copy(f'/content/drive/MyDrive/viton/HR/data/test/cloth-mask/{cloth_id}.jpg', f'/content/drive/MyDrive/viton/HR/data/test/cloth-mask/{image_name}.jpg')\n",
        "\n",
        "\n",
        "\n",
        "image = mp.Image.create_from_file(f\"/content/drive/MyDrive/viton/HR/data/test/image/{image_name}.jpg\")\n",
        "\n",
        "# For Body Landmarks\n",
        "# STEP 2: Create an PoseLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    output_segmentation_masks=True)\n",
        "detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# STEP 4: Detect pose landmarks from the input image.\n",
        "body_detection_result = detector.detect(image)\n",
        "\n",
        "# For Hands_landmarks\n",
        "# STEP 2: Create an HandLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
        "                                       num_hands=2)\n",
        "detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "# STEP 4: Detect hand landmarks from the input image.\n",
        "hands_detection_result = detector.detect(image)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "create_keypoints_json(image.numpy_view(), body_detection_result, hands_detection_result, image_name)\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive/viton/HR\n",
        "!python get_parse_agnostic.py --data_path data/test --output_path data/test/image-parse-agnostic-v3.2\n",
        "!python test_generator.py --occlusion --cuda True --test_name test_name --gpu_ids 0 --datasetting unpaired --dataroot data --data_list test_pairs.txt\n",
        "\n",
        "\n",
        "person = cv2.imread(f\"/content/drive/MyDrive/viton/HR/data/test/image/{image_name}.jpg\")\n",
        "img = cv2.imread(f\"/content/drive/MyDrive/viton/HR/Output/{image_name}_{cloth_id}.png\")\n",
        "cloth = cv2.imread(f\"/content/drive/MyDrive/viton/HR/data/test/cloth/{cloth_id}.jpg\")\n",
        "final_img = np.concatenate([person, cloth, img], axis=1)\n",
        "\n",
        "cv2_imshow(final_img)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-85IKHIRk1o5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}